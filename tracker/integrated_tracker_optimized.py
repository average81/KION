import cv2
import tqdm
import numpy as np
from ultralytics import YOLO
import pandas as pd
from moviepy import VideoFileClip, AudioFileClip
import os
from collections import deque, defaultdict
from typing import List, Tuple, Optional, Dict, Any
import logging
from face_recognition import FaceRecognition
from .base_tracker import BaseVideoProcessor

# –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
import librosa
import librosa.display

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

script_dir = os.path.dirname(os.path.abspath(__file__))

class OptimizedBoundingBoxTracker():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–µ–∫–µ—Ä –æ–±—ä–µ–∫—Ç–æ–≤ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é"""
    
    def __init__(self, max_history: int = 30, interpolation_frames: int = 20, 
                 iou_threshold: float = 0.3, min_detections_for_tracking: int = 6,
                 min_confidence: float = 0.7):
        self.objects: Dict[int, 'TrackedObject'] = {}
        self.next_id = 0
        self.max_history = max_history
        self.interpolation_frames = interpolation_frames
        self.iou_threshold = iou_threshold
        self.min_detections_for_tracking = min_detections_for_tracking
        self.min_confidence = min_confidence
        self.current_frame = 0
        
        # –ö—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        self._iou_cache = {}
        self._frame_cache_size = 100
        
    def update(self, detections: List[List], frame_num: int) -> List[List]:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Ç—Ä–µ–∫–µ—Ä —Å –Ω–æ–≤—ã–º–∏ –¥–µ—Ç–µ–∫—Ü–∏—è–º–∏"""
        self.current_frame = frame_num
        self._clear_old_cache()
        
        if not detections:
            return self._get_all_boxes()
            
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏
        matched_detections, unmatched_detections = self._match_detections(detections)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –æ–±—ä–µ–∫—Ç—ã
        for detection, obj_id in matched_detections:
            self.objects[obj_id].update(detection, frame_num)
            
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã –¥–ª—è –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–π
        for detection in unmatched_detections:
            self._create_new_object(detection, frame_num)
            
        # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –æ–±—ä–µ–∫—Ç—ã
        self._remove_expired_objects()
        
        return self._get_all_boxes()
    
    def _match_detections(self, detections: List[List]) -> Tuple[List[Tuple], List[List]]:
        """–°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ –æ–±—ä–µ–∫—Ç–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É—è IoU –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)"""
        if not self.objects:
            return [], detections
            
        matched = []
        unmatched = []
        
        for detection in detections:
            best_match_id = None
            best_iou = 0.0
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä –Ω–æ–≤–æ–π –¥–µ—Ç–µ–∫—Ü–∏–∏
            new_center_x = (detection[0] + detection[2]) / 2
            new_center_y = (detection[1] + detection[3]) / 2
            
            for obj_id, obj in self.objects.items():
                last_box = obj.get_last_box()
                if last_box is not None:
                    iou = self._calculate_iou(tuple(detection[:4]), tuple(last_box))
                    
                    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏
                    last_center_x = (last_box[0] + last_box[2]) / 2
                    last_center_y = (last_box[1] + last_box[3]) / 2
                    distance = np.sqrt((new_center_x - last_center_x)**2 + (new_center_y - last_center_y)**2)
                    
                    # –£—Å–ª–æ–≤–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
                    if (iou > best_iou and iou > self.iou_threshold and distance < 100):
                        best_iou = iou
                        best_match_id = obj_id
                        logger.debug(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: –æ–±—ä–µ–∫—Ç {obj_id}, IoU={iou:.3f}, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ={distance:.1f}")
            
            if best_match_id is not None:
                matched.append((detection, best_match_id))
            else:
                unmatched.append(detection)
        
        return matched, unmatched
    
    def _create_new_object(self, detection: List, frame_num: int):
        """–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è"""
        obj = TrackedObject(
            self.next_id, detection, frame_num, 
            self.max_history, self.interpolation_frames
        )
        self.objects[self.next_id] = obj
        self.next_id += 1
        logger.info(f"üÜï –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –æ–±—ä–µ–∫—Ç {obj.object_id}")
    
    def _remove_expired_objects(self):
        """–£–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –æ–±—ä–µ–∫—Ç—ã"""
        expired_objects = []
        
        for obj_id, obj in self.objects.items():
            frames_since_creation = self.current_frame - obj.creation_frame
            frames_since_seen = self.current_frame - obj.last_seen
            
            # –£–¥–∞–ª—è–µ–º –æ–±—ä–µ–∫—Ç—ã —Å—Ç–∞—Ä—à–µ 300 –∫–∞–¥—Ä–æ–≤ (~12 —Å–µ–∫—É–Ω–¥)
            if frames_since_creation > 300:
                expired_objects.append(obj_id)
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π –æ–±—ä–µ–∫—Ç {obj_id} (–≤–æ–∑—Ä–∞—Å—Ç: {frames_since_creation} –∫–∞–¥—Ä–æ–≤)")
                continue
                
            # –£–¥–∞–ª—è–µ–º –æ–±—ä–µ–∫—Ç—ã –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–µ –±–æ–ª–µ–µ 25 –∫–∞–¥—Ä–æ–≤ (~1 —Å–µ–∫—É–Ω–¥–∞) –¥–ª—è –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã—Ö (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ)
            if not obj.is_tracking and frames_since_seen > 25:
                expired_objects.append(obj_id)
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –Ω–µ–∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—ä–µ–∫—Ç {obj_id} (–¥–µ—Ç–µ–∫—Ü–∏–π: {obj.detection_count}, –Ω–µ–∞–∫—Ç–∏–≤–µ–Ω: {frames_since_seen} –∫–∞–¥—Ä–æ–≤)")
                continue
            # –£–¥–∞–ª—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã —á–µ—Ä–µ–∑ 20 –∫–∞–¥—Ä–æ–≤ (~0.8 —Å–µ–∫—É–Ω–¥—ã)
            elif obj.is_tracking and frames_since_seen > 20:
                expired_objects.append(obj_id)
                logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω –∞–∫—Ç–∏–≤–Ω—ã–π –æ–±—ä–µ–∫—Ç {obj_id} (–Ω–µ–∞–∫—Ç–∏–≤–µ–Ω: {frames_since_seen} –∫–∞–¥—Ä–æ–≤)")
                continue
        
        for obj_id in expired_objects:
            del self.objects[obj_id]
    
    def _get_all_boxes(self) -> List[List]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Ä–∞–º–∫–∏"""
        active_boxes = []
        
        for obj in self.objects.values():
            if obj.is_tracking:
                obj.check_interpolation(self.current_frame)
                box, conf = obj.get_smoothed_box()
                
                if box is None:
                    box, conf = obj.get_interpolated_box(self.current_frame)
                
                # Fallback –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–º–∫—É –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                if box is None and len(obj.history) > 0:
                    last_box, last_conf = obj.history[-1]
                    box = last_box[:4]
                    conf = last_conf
                    logger.debug(f"üîÑ Fallback –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {obj.object_id}")
                
                if box is not None:
                    active_boxes.append([*box, conf, obj.object_id])
                    logger.debug(f"‚úÖ –û–±—ä–µ–∫—Ç {obj.object_id} –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è (–∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è: {obj.is_interpolating})")
        
        return active_boxes
    
    def _calculate_distance(self, box1: Tuple, box2: Tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏ —Ä–∞–º–æ–∫"""
        center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)
        center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)
        return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _calculate_iou(self, box1: Tuple, box2: Tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç IoU –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–∞–º–∫–∞–º–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        cache_key = tuple(sorted([box1, box2]))
        if cache_key in self._iou_cache:
            return self._iou_cache[cache_key]
        
        x1, y1, x2, y2 = box1
        x3, y3, x4, y4 = box2
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        x_left = max(x1, x3)
        y_top = max(y1, y3)
        x_right = min(x2, x4)
        y_bottom = min(y2, y4)
        
        if x_right < x_left or y_bottom < y_top:
            iou = 0.0
        else:
            intersection = (x_right - x_left) * (y_bottom - y_top)
            area1 = (x2 - x1) * (y2 - y1)
            area2 = (x4 - x3) * (y4 - y3)
            union = area1 + area2 - intersection
            iou = intersection / union if union > 0 else 0.0
        
        self._iou_cache[cache_key] = iou
        return iou
    
    def _clear_old_cache(self):
        """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–π –∫—ç—à –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"""
        if len(self._iou_cache) > self._frame_cache_size:
            self._iou_cache.clear()
    
    def reset(self):
        """–°–±—Ä–∞—Å—ã–≤–∞–µ—Ç —Ç—Ä–µ–∫–µ—Ä –ø—Ä–∏ —Å–º–µ–Ω–µ —Å—Ü–µ–Ω—ã"""
        self.objects.clear()
        self.next_id = 0
        self._iou_cache.clear()
        logger.info("üîÑ –¢—Ä–µ–∫–µ—Ä —Å–±—Ä–æ—à–µ–Ω")


class TrackedObject:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞"""
    
    def __init__(self, object_id: int, initial_detection: Tuple, creation_frame: int,
                 max_history: int, interpolation_frames: int):
        self.object_id = object_id
        self.creation_frame = creation_frame
        self.last_seen = creation_frame
        self.detection_count = 1
        self.is_tracking = False
        self.is_interpolating = False
        self.interpolation_start_frame = 0
        self.interpolation_start = None
        self.interpolation_end = None
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        self.history = deque(maxlen=max_history)
        self.interpolation_frames = interpolation_frames
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–µ—Ä–≤—É—é –¥–µ—Ç–µ–∫—Ü–∏—é
        self.history.append((initial_detection[:4], initial_detection[4]))
        
        # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º —Ç—Ä–µ–∫–∏–Ω–≥ –ø–æ—Å–ª–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–µ—Ç–µ–∫—Ü–∏–π
        self._check_tracking_activation()
    
    def update(self, detection: Tuple, frame_num: int):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ–±—ä–µ–∫—Ç –Ω–æ–≤–æ–π –¥–µ—Ç–µ–∫—Ü–∏–µ–π"""
        self.last_seen = frame_num
        self.detection_count += 1
        self.is_interpolating = False
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.history.append((detection[:4], detection[4]))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏—é —Ç—Ä–µ–∫–∏–Ω–≥–∞
        self._check_tracking_activation()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
        if self.is_tracking:
            last_box = self.get_last_box()
            if last_box is not None:
                distance = self._calculate_distance(detection[:4], last_box)
                logger.debug(f"üìè –û–±—ä–µ–∫—Ç {self.object_id}: —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ {distance:.1f}px, IoU: {self._calculate_iou(detection[:4], last_box):.2f}")
    
    def _check_tracking_activation(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–∫–∏–Ω–≥"""
        if not self.is_tracking and self.detection_count >= 6:  # –ê–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø–æ—Å–ª–µ 6 –¥–µ—Ç–µ–∫—Ü–∏–π –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
            self.is_tracking = True
            logger.info(f"üéØ –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω —Ç—Ä–µ–∫–∏–Ω–≥ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {self.object_id} (–¥–µ—Ç–µ–∫—Ü–∏–π: {self.detection_count})")
        elif not self.is_tracking and self.detection_count % 1 == 0:  # –û—Ç–ª–∞–¥–∫–∞ –∫–∞–∂–¥—É—é –¥–µ—Ç–µ–∫—Ü–∏—é
            logger.info(f"üìä –û–±—ä–µ–∫—Ç {self.object_id}: {self.detection_count} –¥–µ—Ç–µ–∫—Ü–∏–π, —Ç—Ä–µ–∫–∏–Ω–≥: {self.is_tracking}")
    
    def check_interpolation(self, current_frame: int):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –Ω–∞—á–∞—Ç—å –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é"""
        frames_since_seen = current_frame - self.last_seen
        
        # –ù–∞—á–∏–Ω–∞–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é –ø–æ—Å–ª–µ 3 –∫–∞–¥—Ä–æ–≤ –±–µ–∑ –¥–µ—Ç–µ–∫—Ü–∏–∏
        if frames_since_seen > 3 and not self.is_interpolating:
            self.is_interpolating = True
            self.interpolation_start_frame = current_frame
            
            if len(self.history) >= 2:
                recent_boxes = list(self.history)[-2:]
                self.interpolation_start = recent_boxes[-1][:4]
                self.interpolation_end = recent_boxes[-2][:4]
            
            logger.debug(f"üîÑ –ù–∞—á–∞—Ç–∞ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {self.object_id}")
        
        # –ó–∞–≤–µ—Ä—à–∞–µ–º –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—é
        elif self.is_interpolating:
            if frames_since_seen > self.interpolation_frames * 2.5:
                self.is_interpolating = False
                logger.debug(f"‚èπÔ∏è –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {self.object_id}")
    
    def get_smoothed_box(self) -> Tuple[Optional[Tuple], Optional[float]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≥–ª–∞–∂–µ–Ω–Ω—É—é —Ä–∞–º–∫—É"""
        if len(self.history) < 3:
            last_box = self.get_last_box()
            last_conf = self.history[-1][1] if self.history else None
            return last_box, last_conf
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º 3 –∫–∞–¥—Ä–∞–º
        recent_boxes = list(self.history)[-3:]
        smoothed_box = self._average_boxes([box for box, _ in recent_boxes])
        avg_conf = sum(conf for _, conf in recent_boxes) / len(recent_boxes)
        
        return smoothed_box, avg_conf
    
    def get_interpolated_box(self, current_frame: int) -> Tuple[Optional[Tuple], Optional[float]]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Ä–∞–º–∫—É"""
        if not self.is_interpolating or self.interpolation_start is None:
            return None, None
        
        frames_since_start = current_frame - self.interpolation_start_frame
        if frames_since_start > self.interpolation_frames:
            return None, None
        
        # –õ–∏–Ω–µ–π–Ω–∞—è –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è
        progress = frames_since_start / self.interpolation_frames
        interpolated_box = self._interpolate_boxes(self.interpolation_start, self.interpolation_end, progress)
        
        return interpolated_box, 0.5  # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –∏–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫
    
    def get_last_box(self) -> Optional[Tuple]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω—é—é —Ä–∞–º–∫—É"""
        return self.history[-1][0] if self.history else None
    
    def _average_boxes(self, boxes: List[Tuple]) -> Tuple:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ä–µ–¥–Ω—é—é —Ä–∞–º–∫—É"""
        if not boxes:
            return None
        
        avg_box = [0, 0, 0, 0]
        for box in boxes:
            for i in range(4):
                avg_box[i] += box[i]
        
        return tuple(x / len(boxes) for x in avg_box)
    
    def _interpolate_boxes(self, box1: Tuple, box2: Tuple, progress: float) -> Tuple:
        """–ò–Ω—Ç–µ—Ä–ø–æ–ª–∏—Ä—É–µ—Ç –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–∞–º–∫–∞–º–∏"""
        return tuple(b1 + (b2 - b1) * progress for b1, b2 in zip(box1, box2))
    
    def _calculate_distance(self, box1: Tuple, box2: Tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏ —Ä–∞–º–æ–∫"""
        center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)
        center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)
        return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _calculate_iou(self, box1: Tuple, box2: Tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç IoU –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–∞–º–∫–∞–º–∏"""
        x1, y1, x2, y2 = box1
        x3, y3, x4, y4 = box2
        
        x_left = max(x1, x3)
        y_top = max(y1, y3)
        x_right = min(x2, x4)
        y_bottom = min(y2, y4)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = (x2 - x1) * (y2 - y1)
        area2 = (x4 - x3) * (y4 - y3)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0


class OptimizedSceneDetector:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä —Å–º–µ–Ω —Å—Ü–µ–Ω"""
    
    def __init__(self, threshold: float = 10.0, min_scene_length: float = 0.7, 
                 required_consecutive: int = 1):
        self.threshold = threshold
        self.min_scene_length = min_scene_length
        self.required_consecutive = required_consecutive
        self.scene_changes = []
        self.prev_frame = None
        self.consecutive_detections = 0
        self.last_scene_change = 0

    def final_scene(self, frame_num: int):
        self.scene_changes.append(frame_num)

        
    def detect_scene_change(self, frame: np.ndarray, frame_num: int, fps: float) -> bool:
        """–î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç —Å–º–µ–Ω—É —Å—Ü–µ–Ω—ã"""
        if self.prev_frame is None:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            self.prev_frame = gray
            return False
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–æ—Å—Ç—å –∫–∞–¥—Ä–æ–≤ (–∫–∞–∫ –≤ –Ω–µ–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        diff = cv2.absdiff(gray, self.prev_frame)
        mean_diff = np.mean(diff)
        normalized_diff = (mean_diff / 255.0) * 100  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É —Å—Ü–µ–Ω—ã
        min_frames = int(self.min_scene_length * fps)
        if frame_num - self.last_scene_change < min_frames:
            self.prev_frame = gray
            return False
        
        # –î–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —Å–º–µ–Ω—É —Å—Ü–µ–Ω—ã (–∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Ä–∞–∑–Ω–æ—Å—Ç—å)
        if normalized_diff > self.threshold:
            self.consecutive_detections += 1
            if self.consecutive_detections >= self.required_consecutive:
                self.scene_changes.append(frame_num)
                self.last_scene_change = frame_num
                self.consecutive_detections = 0
                logger.info(f"üé¨ –°–º–µ–Ω–∞ —Å—Ü–µ–Ω—ã –≤ –∫–∞–¥—Ä–µ {frame_num} (—Ä–∞–∑–Ω–æ—Å—Ç—å: {normalized_diff:.1f})")
                self.prev_frame = gray
                return True
        else:
            self.consecutive_detections = 0
        
        self.prev_frame = gray
        return False
    
    def get_current_scene(self, frame_num: int) -> int:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–º–µ—Ä —Ç–µ–∫—É—â–µ–π —Å—Ü–µ–Ω—ã"""
        scene_num = 0
        for change_frame in self.scene_changes:
            if frame_num >= change_frame:
                scene_num += 1
        return scene_num
    
    def save_analysis(self, output_file: str, fps: float):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∞–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω –≤ CSV"""
        if not self.scene_changes:
            return
        
        data = []
        prev_frame = 0
        
        for i, change_frame in enumerate(self.scene_changes):
            duration = (change_frame - prev_frame) / fps
            data.append({
                'scene': i,
                'start_frame': prev_frame,
                'start_time': prev_frame / fps,
                'end_frame': change_frame,
                'end_time': change_frame / fps,
                'duration_seconds': duration
            })
            prev_frame = change_frame
        

        
        df = pd.DataFrame(data)
        df.to_csv(output_file, index=False)
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —à–æ—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {output_file}")


class OptimizedVideoProcessor(BaseVideoProcessor):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –≤–∏–¥–µ–æ"""
    
    def __init__(self, model_path: str = script_dir + "/yolov8n.pt", detector = 'mmod', force_update = False):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        super().__init__(model_path, detector, force_update)
        self.scene_detector = OptimizedSceneDetector(threshold = 15)
        self.tracker = OptimizedBoundingBoxTracker()

    def process_video(self, video_path: str, output_path: str):
        self.video_path = video_path
        self.output_path = output_path if output_path else ""
        if not os.path.exists(self.video_path):
            logger.error(f"üö® –í–∏–¥–µ–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {self.video_path}")
            return

        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π, —Ç—Ä–µ–∫–∏–Ω–≥–æ–º –∏ —Å–º–µ–Ω–æ–π —Å—Ü–µ–Ω"""
        self.clip = VideoFileClip(self.video_path)

        length = self.clip.duration
        width = self.clip.size[0]
        height = self.clip.size[1]
        total_frames = self.clip.n_frames
        fps = total_frames/length

        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        shorts_path = output_path + '/shorts'
        if not os.path.exists(shorts_path):
            os.makedirs(shorts_path)
        prefix = 'input_short_'

        logger.info(f"üé¨ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ: {total_frames} –∫–∞–¥—Ä–æ–≤, {fps:.1f} FPS")
        video_scenes = []   #–ù–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü —Å—Ü–µ–Ω –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        scene_start_frame = 0
        frame_count = 0
        self.names = {}
        shapes_df = pd.DataFrame(columns=['id', 'frame', 'shape', 'face_desc'])
        #–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤
        subtitles_path = video_path[:-4] + '.srt'
        subtitles_scenes = self.analyze_subtitles(subtitles_path, video_path)

        while frame_count < total_frames:
            frame = self.clip.get_frame(frame_count/self.clip.fps)

            frame_count += 1
            
            # –î–µ—Ç–µ–∫—Ü–∏—è —Å–º–µ–Ω—ã —Å—Ü–µ–Ω—ã
            scene_changed = self.scene_detector.detect_scene_change(frame, frame_count, fps)
            if scene_changed:
                self.names = {}  # –°–±—Ä–æ—Å –∏–º–µ–Ω –ø–æ—Å–ª–µ —Å–º–µ–Ω—ã —Å—Ü–µ–Ω—ã
                self.tracker.reset()
                scene = (scene_start_frame/fps, (frame_count-1)/fps)
                video_scenes.append(scene)
                short_path = os.path.join(shorts_path, f"{prefix}{len(video_scenes)}.mp4")
                clip = VideoFileClip(video_path).subclipped(scene[0] + 1/2/fps, scene[1])
                clip.write_videofile(short_path, codec='libx264', audio_codec='aac')
                clip.close()
                logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ü–µ–Ω–∞ {len(video_scenes)} –≤ {short_path}")
                scene_start_frame = frame_count
                self.shapes_list.append(shapes_df)
                shapes_df = pd.DataFrame(columns=['id', 'frame', 'shape', 'face_desc'])
                # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ
                #self._attach_audio(f"{prefix}{len(video_scenes) + 1}tmp.mp4", f"{prefix}{len(video_scenes) + 1}.mp4")
                #out.release()
                #output_path = os.path.join(shorts_path, f"{prefix}{len(video_scenes) + 1}tmp.mp4")
                #out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
            # YOLO –¥–µ—Ç–µ–∫—Ü–∏—è (–∫–∞–∂–¥—ã–π –∫–∞–¥—Ä)
            results = self.model(frame, conf=0.6, iou=0.35, max_det=15, verbose=False)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–π –ª—é–¥–µ–π
            detections = []
            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        if box.cls == 0:  # –∫–ª–∞—Å—Å "person"
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            conf = box.conf[0].cpu().numpy()
                            detections.append([x1, y1, x2, y2, conf])
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            detections = self._filter_duplicates(detections)
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–µ—Ä–∞
            tracked_boxes = self.tracker.update(detections, frame_count)
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ü
            if tracked_boxes:
                for box in tracked_boxes:
                    x1, y1, x2, y2 = box[:4]
                    x1 = int(x1)
                    y1 = int(y1)
                    x2 = int(x2)
                    y2 = int(y2)
                    obj_id = box[5]

                    if obj_id not in self.names:
                        #–≤—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–µ–Ω—è–µ–º BGR –Ω–∞ RGB
                        human_img = np.array(frame[y1:y2, x1:x2, ::-1], dtype=np.uint8)
                        face = self.face_recognition.detector.shape_of_image(human_img)
                        if face is None:
                            description = None
                        else:
                            description = self.face_recognition.facerec.compute_face_descriptor(human_img, face)
                        box.append(face)
                        if description is not None:
                            name = self.face_recognition.find_name_desc(description)
                            if name != "Unknown":
                                if len(self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] == name]) < 10:
                                    #print(len(self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] == name]))
                                    self.face_recognition.add_face_desc(description, name)
                                    logging.info(f"üë§ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ª–∏—Ü–æ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {name}")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–∫–∞—Ö
            if len(tracked_boxes) > 0:
                for det in tracked_boxes:
                    x1 = int(det[0])
                    y1 = int(det[1])
                    x2 = int(det[2])
                    y2 = int(det[3])
                    track_id = det[5]
                    shapes_df.loc[len(shapes_df)] = [int(track_id),frame_count-scene_start_frame,(x1, y1, x2, y2),det[6]]
        self.scene_detector.final_scene(total_frames - 1)
        #–£–¥–∞–ª—è–µ–º –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–ø–∏—Å–∏, —Å –∏–º–µ–Ω–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–æ–∑–Ω–∞–Ω—ã –º–µ–Ω–µ–µ, —á–µ–º –≤ 10 –∫–∞–¥—Ä–∞—Ö
        for name in self.face_recognition.local_dataset['name'].unique():
            if len(self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] == name]) < 10:
                self.face_recognition.local_dataset = self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] != name]
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —à–æ—Ç–æ–≤, 2 –ø—Ä–æ—Ö–æ–¥...")
        if not os.path.exists(output_path + f'/shaped_shorts'):
            os.makedirs(output_path + f'/shaped_shorts')
        for i, (start, end) in enumerate(video_scenes):
            short_clip = self.clip.subclipped(start, end)
            self.video_short_tracker(short_clip, output_path + f'/shaped_shorts/input_debug_{i}.mp4', i)


        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è
        video_weight = 0.7  # –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è –≤–∏–¥–µ–æ —Å—Ü–µ–Ω
        audio_weight = 0.3  # –º–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è –∞—É–¥–∏–æ —Å—Ü–µ–Ω
        min_duration = 10.0  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        max_duration = 600.0 # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        merged_scenes = None
        logger.info(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(video_scenes)} —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ –∞–Ω–∞–ª–∏–∑–∞:")
        for i, (start, end) in enumerate(video_scenes):
            logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫")

        if self.clip.audio is not None:
            self.clip.audio.write_audiofile(output_path + '/sound.wav', codec='pcm_s16le')
            audio_features = self.analyze_audio()
            audio_scenes = self.detect_audio_scenes(audio_features)
            logger.info(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(audio_scenes)} —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ –∞–Ω–∞–ª–∏–∑–∞:")
            for i, (start, end) in enumerate(audio_scenes):
                logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫")
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ü–µ–Ω—ã —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            merged_scenes = self.merge_scenes(video_scenes, audio_scenes,
                                              subtitles_scenes,
                                              min_scene_duration=min_duration)
            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(merged_scenes)} —Å—Ü–µ–Ω –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:")
            for i, (start, end) in enumerate(merged_scenes):
                logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {end-start:.2f} —Å–µ–∫)")

        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ü–µ–Ω—ã
        if merged_scenes:
            scene_path = output_path + '/scenes'
            prefix = 'input_scene_'
            self.save_scenes_as_videos(video_path, self.clip.fps, merged_scenes, scene_path,prefix)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞
        self.scene_detector.save_analysis(output_path + "/shot_analysis.csv", fps)

        self.clip.close()

        data = []
        prev_frame = 0

        for i, scene in enumerate(merged_scenes):
            duration = (scene[1] - scene[0])
            change_frame = int(scene[1] * fps)
            data.append({
                'scene': i,
                'start_frame': prev_frame,
                'start_time': prev_frame / fps,
                'end_frame': change_frame,
                'end_time': change_frame / fps,
                'duration_seconds': duration
            })
            prev_frame = change_frame


        df = pd.DataFrame(data)
        scene_analysis_path = output_path + '/scene_analysis.csv'
        df.to_csv(scene_analysis_path, index=False)
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scene_analysis_path}")
        
        logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    
    def _filter_duplicates(self, detections: List[List]) -> List[List]:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        if len(detections) <= 1:
            return detections
        
        filtered = []
        for i, det1 in enumerate(detections):
            is_duplicate = False
            for j, det2 in enumerate(detections):
                if i != j:
                    iou = self._calculate_iou(det1[:4], det2[:4])
                    distance = self._calculate_distance(det1[:4], det2[:4])
                    
                    # –ë–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: IoU > 0.3 –∏–ª–∏ –±–ª–∏–∑–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                    if iou > 0.3 or (iou > 0.1 and distance < 30):
                        # –û—Å—Ç–∞–≤–ª—è–µ–º –¥–µ—Ç–µ–∫—Ü–∏—é —Å –±–æ–ª—å—à–µ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                        if det1[4] < det2[4]:
                            is_duplicate = True
                            logger.debug(f"üö´ –£–¥–∞–ª–µ–Ω –¥—É–±–ª–∏–∫–∞—Ç: IoU={iou:.2f}, —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ={distance:.1f}")
                            break
            if not is_duplicate:
                filtered.append(det1)
        
        return filtered
    
    def _get_color_for_id(self, obj_id: int) -> Tuple[int, int, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è ID –æ–±—ä–µ–∫—Ç–∞"""
        if obj_id not in self.color_cache:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ ID
            hue = (obj_id * 137.5) % 360  # –ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HSV –≤ BGR
            h = hue / 60
            i = int(h)
            f = h - i
            p = 0
            q = 255 * (1 - f)
            t = 255 * f
            
            if i == 0:
                r, g, b = 255, t, p
            elif i == 1:
                r, g, b = q, 255, p
            elif i == 2:
                r, g, b = p, 255, t
            elif i == 3:
                r, g, b = p, q, 255
            elif i == 4:
                r, g, b = t, p, 255
            else:
                r, g, b = 255, p, q
            
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —è—Ä–∫–æ—Å—Ç—å –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
            brightness = 0.8
            r = min(255, int(r * brightness))
            g = min(255, int(g * brightness))
            b = min(255, int(b * brightness))
            
            self.color_cache[obj_id] = (b, g, r)  # BGR —Ñ–æ—Ä–º–∞—Ç –¥–ª—è OpenCV
        
        return self.color_cache[obj_id]

    def _calculate_distance(self, box1: Tuple, box2: Tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–∞–º–∏ —Ä–∞–º–æ–∫"""
        center1 = ((box1[0] + box1[2]) / 2, (box1[1] + box1[3]) / 2)
        center2 = ((box2[0] + box2[2]) / 2, (box2[1] + box2[3]) / 2)
        return np.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)
    
    def _calculate_iou(self, box1: Tuple, box2: Tuple) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç IoU –º–µ–∂–¥—É –¥–≤—É–º—è —Ä–∞–º–∫–∞–º–∏"""
        x1, y1, x2, y2 = box1
        x3, y3, x4, y4 = box2
        
        x_left = max(x1, x3)
        y_top = max(y1, y3)
        x_right = min(x2, x4)
        y_bottom = min(y2, y4)
        
        if x_right < x_left or y_bottom < y_top:
            return 0.0
        
        intersection = (x_right - x_left) * (y_bottom - y_top)
        area1 = (x2 - x1) * (y2 - y1)
        area2 = (x4 - x3) * (y4 - y3)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def _attach_audio(self, input_path: str, output_path: str, scene):
        """–ü—Ä–∏—Å–æ–µ–¥–∏–Ω—è–µ—Ç –∞—É–¥–∏–æ –∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–º—É –≤–∏–¥–µ–æ"""
        try:
            # –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –±–µ–∑ –∞—É–¥–∏–æ
            temp_path = output_path.replace('.mp4', '_temp.mp4')
            os.rename(output_path, temp_path)
            
            # –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∞—É–¥–∏–æ
            video = VideoFileClip(temp_path).subclipped(scene[0], scene[1])
            audio = VideoFileClip(input_path).audio
            
            if audio is not None:
                final_video = video.with_audio(audio)
                final_video.write_videofile(output_path, codec='libx264', audio_codec='aac')
                final_video.close()
                audio.close()
            else:
                video.write_videofile(output_path, codec='libx264', verbose=False)
                video.close()
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            os.remove(temp_path)
            logger.info("üéµ –ê—É–¥–∏–æ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–æ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–∏—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–∏ –∞—É–¥–∏–æ: {e}")
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–∞–π–ª –±–µ–∑ –∞—É–¥–∏–æ
            if os.path.exists(temp_path):
                os.rename(temp_path, output_path)

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    processor = OptimizedVideoProcessor()
    processor.process_video("C:/Users/above/IdeaProjects/video/Video_Samples/in2.mp4", "output_video_optimized.mp4")


if __name__ == "__main__":
    main() 