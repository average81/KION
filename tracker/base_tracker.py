import os
import cv2
import pandas as pd
import numpy as np
from typing import List, Tuple, Optional, Dict, Any

# –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
from moviepy import VideoFileClip
import scenedetect
from scenedetect.detectors import ContentDetector
from scenedetect.scene_manager import SceneManager

# –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
import librosa
import librosa.display

# –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤
from ultralytics import YOLO

from face_recognition import FaceRecognition


script_dir = os.path.dirname(os.path.abspath(__file__))

import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class BaseVideoProcessor:
    def __init__(self, model_path: str = script_dir + "/yolov8n.pt", detector = 'mmod', force_update = False):
        self.model = YOLO(model_path)
        self.color_cache = {}  # –ö—ç—à —Ü–≤–µ—Ç–æ–≤ –¥–ª—è ID
        self.face_recognition = FaceRecognition(detector,recognition_value = 0.4)
        self.face_recognition.load_dataset(tomemory = True, force_update = force_update)
        self.shapes_list = []   #—Å–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –ø–æ —à–æ—Ç–∞–º —Å —Ç—Ä–µ–∫–∞–º–∏ –ª—é–¥–µ–π

    def video_short_pretracker(self, clip, clipnum):
        #–ü—É—Å—Ç—ã—à–∫–∞
        return

    def video_short_tracker(self, clip, output_path: str, clipnum):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –∏ —Ç—Ä–µ–∫–æ–º–∏–Ω–≥–æ–º –æ–±—ä–µ–∫—Ç–æ–≤"""

        logger.info(f"üé¨ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —à–æ—Ç–∞ {clipnum}: {clip.n_frames} –∫–∞–¥—Ä–æ–≤, {clip.fps:.1f} FPS")
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, clip.fps, clip.size)
        total_frames = clip.n_frames
        length = clip.duration
        fps = total_frames/length
        self.names = {}
        #window = dlib.image_window()
        shapes_df = pd.DataFrame(columns=['id', 'frame', 'shape', 'face_desc', 'name'])
        for frame_count in range(clip.n_frames):
            frame = clip.get_frame(frame_count/fps)
            #–ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ pretracker
            detections = self.shapes_list[clipnum].loc[self.shapes_list[clipnum]['frame'] == frame_count]


            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–µ—Ä–∞
            #tracked_boxes = self.tracker.update(detections, frame_count)
            tracked_boxes = detections if len(detections) > 0 else []
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ü
            if len(tracked_boxes) > 0:
                for ind,detection in tracked_boxes.iterrows():
                    shape = detection['shape']
                    x1 = shape[0]
                    y1 = shape[1]
                    x2 = shape[2]
                    y2 = shape[3]
                    obj_id = detection['id']
                    face = detection['face_desc']

                    if obj_id not in self.names:
                        #–≤—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–µ–Ω—è–µ–º BGR –Ω–∞ RGB
                        human_img = np.array(frame[y1:y2, x1:x2], dtype=np.uint8)
                        if face is None:
                            name = 'Unknown'
                        else:
                            desc = self.face_recognition.facerec.compute_face_descriptor(human_img, face)
                            name = self.face_recognition.find_name_desc(desc)
                        #window.wait_for_keypress(' ')
                        if name != "Unknown":
                            self.names[obj_id] = name
                            logging.info(f"üë§ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ª–∏—Ü–æ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {obj_id}: {self.names[obj_id]}")


            #
            if len(detections) > 0:
                for ind,detection in tracked_boxes.iterrows():
                    shapes_df.loc[len(shapes_df)] = detection
        for ind,detection in shapes_df.iterrows():
            if detection['id'] in self.names:
                name = self.names[detection['id']]
            else:
                name = str(detection['id'])
            shapes_df['name'].loc[ind] = name

        for frame_count in range(clip.n_frames):
            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            frame = clip.get_frame(frame_count/fps)[:,:,::-1]
            boxes = shapes_df[shapes_df['frame'] == frame_count]
            processed_frame = self._draw_results(frame, boxes, frame_count)
            # –ó–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞
            out.write(processed_frame)

        out.release()
        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ç—Ä–µ–∫–∞–º–∏
        shapes_df = shapes_df.drop(columns=['face_desc'])
        shapes_df.to_csv(output_path[:-4] + '_shapes.csv', index=False)
        return

    def process_video(self, video_path: str, output_path: str):
        #–ü—É—Å—Ç—ã—à–∫–∞
        return


    def save_scenes_as_videos(self, video_path, fps, final_scenes, output_dir='./scenes',prefix:str = ''):
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        for i, scene in enumerate(final_scenes):
            output_path = os.path.join(output_dir, f"{prefix}{i+1}.mp4")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º moviepy –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–∏—è —Å—Ü–µ–Ω—ã
            clip = VideoFileClip(video_path).subclipped(scene[0] + 2/fps, scene[1] - 1/3/fps)
            clip.write_videofile(output_path, codec='libx264', audio_codec='aac')
            clip.close()
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ü–µ–Ω–∞ {i+1} –≤ {output_path}")

    def analyze_audio(self, frame_length=2048, hop_length=512):
        if self.clip.audio is None:
            logger.error("üö® –í–∏–¥–µ–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—É–¥–∏–æ")
            return
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        y, sr = librosa.load(self.output_path + '/sound.wav')
        #fps = self.clip.audio.fps
        #y, sr = np.array(self.clip.audio.to_soundarray(fps = fps)*(2**16), dtype = "int16"), fps

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ RMS —ç–Ω–µ—Ä–≥–∏–∏
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        rms_normalized = (rms - np.min(rms)) / (np.max(rms) - np.min(rms))

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=frame_length, hop_length=hop_length)[0]
        spectral_centroid_normalized = (spectral_centroid - np.min(spectral_centroid)) / (np.max(spectral_centroid) - np.min(spectral_centroid))

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        times = librosa.times_like(rms, sr=sr, hop_length=hop_length, n_fft=frame_length)

        return {
            'times': times,
            'rms': rms_normalized,
            'spectral_centroid': spectral_centroid_normalized,
            'zcr': zcr
        }

    def detect_audio_scenes(self, audio_features, rms_threshold=0.3, centroid_threshold=0.4, min_scene_duration=3.0):
        times = audio_features['times']
        rms = audio_features['rms']
        spectral_centroid = audio_features['spectral_centroid']

        scenes = []
        current_scene_start = 0.0

        for i in range(1, len(times)):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ RMS –∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥–µ
            rms_change = abs(rms[i] - rms[i-1])
            centroid_change = abs(spectral_centroid[i] - spectral_centroid[i-1])

            # –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ, —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–µ–π —Å—Ü–µ–Ω—ã
            if rms_change > rms_threshold or centroid_change > centroid_threshold:
                scene_duration = times[i] - current_scene_start
                if scene_duration >= min_scene_duration:
                    scenes.append((current_scene_start, times[i]))
                    current_scene_start = times[i]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ü–µ–Ω—É
        if current_scene_start < times[-1]:
            scenes.append((current_scene_start, times[-1]))

        return scenes

    def merge_scenes(self,video_scenes, audio_scenes,
                     video_weight=0.7, audio_weight=0.3,
                     min_scene_duration=3.0, max_scene_duration=120.0,
                     overlap_threshold=2.0):
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å—Ü–µ–Ω —Å —É—á–µ—Ç–æ–º:
        - –≤–µ—Å–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ
        - –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ü–µ–Ω
        - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –≤–∑–≤–µ—à–∏–≤–∞–µ–º —Å—Ü–µ–Ω—ã
        weighted_scenes = []
        merged_scenes = []
        '''
        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ —Å—Ü–µ–Ω—ã —Å –≤–µ—Å–æ–º
        for start, end in video_scenes:
            duration = end - start
            if duration >= 0:#min_scene_duration:
                weighted_scenes.append({
                    'start': start,
                    'end': end,
                    'weight': video_weight,
                    'type': 'video'
                })

        # –î–æ–±–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ —Å—Ü–µ–Ω—ã —Å –≤–µ—Å–æ–º
        for start, end in audio_scenes:
            duration = end - start
            if duration >= 0:#min_scene_duration:
                weighted_scenes.append({
                    'start': start,
                    'end': end,
                    'weight': audio_weight,
                    'type': 'audio'
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å—Ü–µ–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
        weighted_scenes.sort(key=lambda x: x['start'])

        if not weighted_scenes:
            return []

        # –ê–ª–≥–æ—Ä–∏—Ç–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        
        
        current_scene = weighted_scenes[0].copy()
        
        for scene in weighted_scenes[1:]:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏–ª–∏ –±–ª–∏–∑–æ—Å—Ç—å —Å—Ü–µ–Ω
            scene_overlap = (scene['start'] <= current_scene['end'] + overlap_threshold)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            duration_exceeded = (scene['end'] - current_scene['start']) > max_scene_duration

            # –ï—Å–ª–∏ —Å—Ü–µ–Ω—ã –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            if scene_overlap and not duration_exceeded:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ü–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤
                if scene['type'] == 'video' and current_scene['type'] != 'video':
                    current_scene['end'] = scene['end']
                    current_scene['weight'] += scene['weight']
                    current_scene['type'] = 'mixed'
                elif scene['weight'] > current_scene['weight']:
                    current_scene['end'] = scene['end']
                    current_scene['weight'] += scene['weight']
                    if scene['type'] != current_scene['type']:
                        current_scene['type'] = 'mixed'
                else:
                    if scene['type'] != 'audio' and current_scene['type'] != 'audio':
                        current_scene['end'] = max(current_scene['end'], scene['end'])
                    elif scene['type'] != 'audio':
                        current_scene['end'] = scene['end']
                    elif current_scene['type'] != 'audio':
                        current_scene['end'] = current_scene['end'], scene['end']
                    current_scene['weight'] += scene['weight'] * 0.5  # –º–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ü–µ–Ω—É –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é
                if current_scene['end'] - current_scene['start'] >= min_scene_duration:
                    merged_scenes.append((current_scene['start'], current_scene['end']))
                current_scene = scene.copy()
        '''
        current_scene = [0,0]
        num_video_scene = 0
        num_audio_scene = 0
        while num_video_scene < len(video_scenes) and num_audio_scene < len(audio_scenes):
            if audio_scenes[num_audio_scene][1] > video_scenes[num_video_scene][1]:
                #–∫–æ–Ω–µ—Ü –≤–∏–¥–µ–æ—Å—Ü–µ–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∞—É–¥–∏–æ—Å—Ü–µ–Ω–µ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                current_scene[1] = video_scenes[num_video_scene][1]
                num_video_scene += 1
            elif audio_scenes[num_audio_scene][1] > video_scenes[num_video_scene][1] - overlap_threshold:
                #–∫–æ–Ω–µ—Ü –≤–∏–¥–µ–æ—Å—Ü–µ–Ω—ã –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—É –∞—É–¥–∏–æ—Å—Ü–µ–Ω—ã –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –≤—Ä–µ–º—è - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
                current_scene[1] = video_scenes[num_video_scene][1]
                num_video_scene += 1
                num_audio_scene += 1
                if current_scene[1] - current_scene[0] > min_scene_duration:
                    merged_scenes.append([current_scene[0], current_scene[1]])
                    current_scene = [current_scene[1] + 0.01,current_scene[1] + 0.01]
            elif audio_scenes[num_audio_scene][1] >= video_scenes[num_video_scene][0] + overlap_threshold:
                #–ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≥—Ä–∞–Ω–∏—Ü - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                current_scene[1] = video_scenes[num_video_scene][1]
                num_video_scene += 1
                num_audio_scene += 1
            elif current_scene[1] - current_scene[0] < min_scene_duration:
                #–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ü–µ–Ω–∞ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                current_scene[1] = video_scenes[num_video_scene][1]
                num_video_scene += 1
                if audio_scenes[num_audio_scene][1] < video_scenes[num_video_scene][0]:
                    num_audio_scene += 1
            else:
                #–§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Å—Ü–µ–Ω—É
                merged_scenes.append([current_scene[0], current_scene[1]])
                num_audio_scene += 1
                current_scene = [current_scene[1] + 0.01,current_scene[1] + 0.01]
        merged_scenes.append([current_scene[0], current_scene[1]])
        #–ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ü–µ–Ω–∞
        if current_scene[1] < video_scenes[-1][1]:
            if video_scenes[-1][1] - current_scene[1] > min_scene_duration:
                merged_scenes.append([current_scene[0], video_scenes[-1][1]])
            else:
                merged_scenes[-1][1] = video_scenes[-1][1]





        ''' 
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ü–µ–Ω—É
        if current_scene['end'] - current_scene['start'] >= min_scene_duration:
            merged_scenes.append((current_scene['start'], video_scenes[-1][1])) #–î–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ü–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        merged_scenes = [(start, end) for start, end in merged_scenes
                         if end - start >= min_scene_duration]
        '''
        return merged_scenes

    def _draw_results(self, frame: np.ndarray, tracked_boxes: pd.DataFrame,
                      frame_num: int) -> np.ndarray:
        """–û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∫–∞–¥—Ä–µ"""
        result_frame = frame.copy()

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if frame_num % 30 == 0:  # –ö–∞–∂–¥—ã–µ 30 –∫–∞–¥—Ä–æ–≤
            logger.debug(f"üé® –û—Ç—Ä–∏—Å–æ–≤–∫–∞: {len(tracked_boxes)} –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞–¥—Ä–µ {frame_num}")

        # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–æ–∫ —Å —Ä–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω—ã–º–∏ ID
        for row in tracked_boxes.itertuples():
            _,obj_id,_,shape,_,_ = row
            x1, y1, x2, y2 = shape
            if obj_id == None:
                continue
            color = self._get_color_for_id(obj_id)

            # –†–∞–º–∫–∞
            cv2.rectangle(result_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)

            # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            #text = f"ID: {obj_id}"
            text = self.names[obj_id] if obj_id in self.names else f"ID: {obj_id}"
            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
            cv2.rectangle(result_frame, (int(x1), int(y1)),
                          (int(x1)+text_width+10, int(y1)+text_height+10), color, -1)

            # –¢–µ–∫—Å—Ç ID
            cv2.putText(result_frame, text, (int(x1)+5, int(y1)+20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            #conf_text = f"{conf:.2f}"
            #cv2.putText(result_frame, conf_text, (int(x1), int(y2)-5),
            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return result_frame

    def _get_color_for_id(self, obj_id: int) -> Tuple[int, int, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è ID –æ–±—ä–µ–∫—Ç–∞"""
        if obj_id not in self.color_cache:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ ID
            hue = (obj_id * 137.5) % 360  # –ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HSV –≤ BGR
            h = hue / 60
            i = int(h)
            f = h - i
            p = 0
            q = 255 * (1 - f)
            t = 255 * f

            if i == 0:
                r, g, b = 255, t, p
            elif i == 1:
                r, g, b = q, 255, p
            elif i == 2:
                r, g, b = p, 255, t
            elif i == 3:
                r, g, b = p, q, 255
            elif i == 4:
                r, g, b = t, p, 255
            else:
                r, g, b = 255, p, q

            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —è—Ä–∫–æ—Å—Ç—å –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
            brightness = 0.8
            r = min(255, int(r * brightness))
            g = min(255, int(g * brightness))
            b = min(255, int(b * brightness))

            self.color_cache[obj_id] = (b, g, r)  # BGR —Ñ–æ—Ä–º–∞—Ç –¥–ª—è OpenCV

        return self.color_cache[obj_id]
