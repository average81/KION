import os
import cv2
import pandas as pd
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import tqdm

# –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
from moviepy import VideoFileClip


# –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
import librosa
import librosa.display

# –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤
from ultralytics import YOLO

from face_recognition import FaceRecognition

import re
from dataclasses import dataclass
from statistics import mean
import spacy
import subprocess

WEIGHTS = {
    'video': 0.5,               # –≤–µ—Å –¥–ª—è –≤–∏–¥–µ–æ-–∞–Ω–∞–ª–∏–∑–∞ (ContentDetector)
    'audio': 0.3,               # –≤–µ—Å –¥–ª—è –∞—É–¥–∏–æ-–∞–Ω–∞–ª–∏–∑–∞
    'subtitles': 0.2,           # –≤–µ—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤
    'min_duration': 10.0,        # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã (—Å–µ–∫)
    'max_duration': 600.0,      # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã (—Å–µ–∫)
    'rms_threshold': 0.3,       # –ø–æ—Ä–æ–≥ –¥–ª—è RMS —ç–Ω–µ—Ä–≥–∏–∏ –≤ –∞—É–¥–∏–æ
    'centroid_threshold': 0.4,  # –ø–æ—Ä–æ–≥ –¥–ª—è —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞
    'sub_min_duration': 10.0,   # –º–∏–Ω. –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –ø–æ —Å—É–±—Ç–∏—Ç—Ä–∞–º
    'sub_time_gap': 3.0,        # –º–∞–∫—Å. —Ä–∞–∑—Ä—ã–≤ –º–µ–∂–¥—É —Ä–µ–ø–ª–∏–∫–∞–º–∏ (—Å–µ–∫)
    'sub_similarity': 0.55,     # –ø–æ—Ä–æ–≥ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞
}


@dataclass
class SubtitleLine:
    index: int
    start: float  # –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    end: float    # –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    text: str
    doc: any = None  # spaCy Doc –æ–±—ä–µ–∫—Ç

script_dir = os.path.dirname(os.path.abspath(__file__))

import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class BaseVideoProcessor:
    def __init__(self, model_path: str = script_dir + "/yolov8n.pt", detector = 'mmod', force_update = False):
        self.model = YOLO(model_path)
        self.color_cache = {}  # –ö—ç—à —Ü–≤–µ—Ç–æ–≤ –¥–ª—è ID
        self.face_recognition = FaceRecognition(detector,recognition_value = 0.4)
        self.face_recognition.load_dataset(tomemory = True, force_update = force_update)
        self.shapes_list = []   #—Å–ø–∏—Å–æ–∫ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–æ–≤ –ø–æ —à–æ—Ç–∞–º —Å —Ç—Ä–µ–∫–∞–º–∏ –ª—é–¥–µ–π
        self.nlp = spacy.load("ru_core_news_md")


    def video_short_pretracker(self, clip, clipnum):
        #–ü—É—Å—Ç—ã—à–∫–∞
        return

    def video_short_tracker(self, clip, output_path: str, clipnum):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –∏ —Ç—Ä–µ–∫–æ–º–∏–Ω–≥–æ–º –æ–±—ä–µ–∫—Ç–æ–≤"""

        logger.info(f"üé¨ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —à–æ—Ç–∞ {clipnum}: {clip.n_frames} –∫–∞–¥—Ä–æ–≤, {clip.fps:.1f} FPS")
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, clip.fps, clip.size)
        total_frames = clip.n_frames
        length = clip.duration
        fps = total_frames/length
        self.names = {}
        #window = dlib.image_window()
        shapes_df = pd.DataFrame(columns=['id', 'frame', 'shape', 'face_desc', 'name'])
        for frame_count in range(clip.n_frames):
            frame = clip.get_frame(frame_count/fps)
            #–ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ pretracker
            detections = self.shapes_list[clipnum].loc[self.shapes_list[clipnum]['frame'] == frame_count]


            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–µ—Ä–∞
            #tracked_boxes = self.tracker.update(detections, frame_count)
            tracked_boxes = detections if len(detections) > 0 else []
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ü
            if len(tracked_boxes) > 0:
                for ind,detection in tracked_boxes.iterrows():
                    shape = detection['shape']
                    x1 = shape[0]
                    y1 = shape[1]
                    x2 = shape[2]
                    y2 = shape[3]
                    obj_id = detection['id']
                    face = detection['face_desc']

                    if obj_id not in self.names:
                        #–≤—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–µ–Ω—è–µ–º BGR –Ω–∞ RGB
                        human_img = np.array(frame[y1:y2, x1:x2], dtype=np.uint8)
                        if face is None:
                            name = 'Unknown'
                        else:
                            desc = self.face_recognition.facerec.compute_face_descriptor(human_img, face)
                            name = self.face_recognition.find_name_desc(desc)
                        #window.wait_for_keypress(' ')
                        if name != "Unknown":
                            self.names[obj_id] = name
                            logging.info(f"üë§ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ª–∏—Ü–æ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {obj_id}: {self.names[obj_id]}")


            #
            if len(detections) > 0:
                for ind,detection in tracked_boxes.iterrows():
                    shapes_df.loc[len(shapes_df)] = detection
        for ind,detection in shapes_df.iterrows():
            if detection['id'] in self.names:
                name = self.names[detection['id']]
            else:
                name = str(detection['id'])
            shapes_df['name'].loc[ind] = name

        for frame_count in range(clip.n_frames):
            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            frame = clip.get_frame(frame_count/fps)[:,:,::-1]
            boxes = shapes_df[shapes_df['frame'] == frame_count]
            processed_frame = self._draw_results(frame, boxes, frame_count)
            # –ó–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞
            out.write(processed_frame)

        out.release()
        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É —Å —Ç—Ä–µ–∫–∞–º–∏
        shapes_df = shapes_df.drop(columns=['face_desc'])
        shapes_df.to_csv(output_path[:-4] + '_shapes.csv', index=False)
        return

    def process_video(self, video_path: str, output_path: str):
        #–ü—É—Å—Ç—ã—à–∫–∞
        return


    def save_scenes_as_videos(self, video_path, fps, final_scenes, output_dir='./scenes',prefix:str = ''):
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        for i, scene in enumerate(final_scenes):
            output_path = os.path.join(output_dir, f"{prefix}{i+1}.mp4")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º moviepy –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–∏—è —Å—Ü–µ–Ω—ã
            clip = VideoFileClip(video_path).subclipped(scene[0] + 2/fps, scene[1] - 1/3/fps)
            clip.write_videofile(output_path, codec='libx264', audio_codec='aac')
            clip.close()
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ü–µ–Ω–∞ {i+1} –≤ {output_path}")

    def analyze_audio(self, frame_length=2048, hop_length=512):
        if self.clip.audio is None:
            logger.error("üö® –í–∏–¥–µ–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—É–¥–∏–æ")
            return
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        y, sr = librosa.load(self.output_path + '/sound.wav')
        #fps = self.clip.audio.fps
        #y, sr = np.array(self.clip.audio.to_soundarray(fps = fps)*(2**16), dtype = "int16"), fps

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ RMS —ç–Ω–µ—Ä–≥–∏–∏
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        rms_normalized = (rms - np.min(rms)) / (np.max(rms) - np.min(rms))

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=frame_length, hop_length=hop_length)[0]
        spectral_centroid_normalized = (spectral_centroid - np.min(spectral_centroid)) / (np.max(spectral_centroid) - np.min(spectral_centroid))

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        times = librosa.times_like(rms, sr=sr, hop_length=hop_length, n_fft=frame_length)

        return {
            'times': times,
            'rms': rms_normalized,
            'spectral_centroid': spectral_centroid_normalized,
            'zcr': zcr
        }

    def detect_audio_scenes(self, audio_features, rms_threshold=0.3, centroid_threshold=0.4, min_scene_duration=3.0):
        times = audio_features['times']
        rms = audio_features['rms']
        spectral_centroid = audio_features['spectral_centroid']

        scenes = []
        current_scene_start = 0.0

        for i in range(1, len(times)):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ RMS –∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥–µ
            rms_change = abs(rms[i] - rms[i-1])
            centroid_change = abs(spectral_centroid[i] - spectral_centroid[i-1])

            # –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ, —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–µ–π —Å—Ü–µ–Ω—ã
            if rms_change > rms_threshold or centroid_change > centroid_threshold:
                scene_duration = times[i] - current_scene_start
                if scene_duration >= min_scene_duration:
                    scenes.append((current_scene_start, times[i]))
                    current_scene_start = times[i]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ü–µ–Ω—É
        if current_scene_start < times[-1]:
            scenes.append((current_scene_start, times[-1]))

        return scenes

    def merge_scenes(self,video_scenes, audio_scenes,
                     subtitle_scenes,
                     min_scene_duration=3.0,
                     overlap_threshold=2.0):
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å—Ü–µ–Ω —Å —É—á–µ—Ç–æ–º:
        - –≤–µ—Å–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ
        - –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ü–µ–Ω
        - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –≤–∑–≤–µ—à–∏–≤–∞–µ–º —Å—Ü–µ–Ω—ã
        weighted_scenes = []
        merged_scenes = []
        current_scene = [0,0]
        num_video_scene = 0
        num_audio_scene = 0
        num_subtitle_scene = 0
        if len(audio_scenes) >0:
            while num_video_scene < len(video_scenes) and num_audio_scene < len(audio_scenes):
                if audio_scenes[num_audio_scene][1] > video_scenes[num_video_scene][1]:
                    #–∫–æ–Ω–µ—Ü –≤–∏–¥–µ–æ—Å—Ü–µ–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∞—É–¥–∏–æ—Å—Ü–µ–Ω–µ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                elif audio_scenes[num_audio_scene][1] > video_scenes[num_video_scene][1] - overlap_threshold:
                    #–∫–æ–Ω–µ—Ü –≤–∏–¥–µ–æ—Å—Ü–µ–Ω—ã –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—É –∞—É–¥–∏–æ—Å—Ü–µ–Ω—ã –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –≤—Ä–µ–º—è - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                    num_audio_scene += 1
                    if current_scene[1] - current_scene[0] > min_scene_duration:
                        merged_scenes.append([current_scene[0], current_scene[1]])
                        current_scene = [current_scene[1] + 0.01,current_scene[1] + 0.01]
                elif audio_scenes[num_audio_scene][1] >= video_scenes[num_video_scene][0] + overlap_threshold:
                    #–ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≥—Ä–∞–Ω–∏—Ü - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                    num_audio_scene += 1
                elif current_scene[1] - current_scene[0] < min_scene_duration:
                    #–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ü–µ–Ω–∞ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                    if audio_scenes[num_audio_scene][1] < video_scenes[num_video_scene][0]:
                        num_audio_scene += 1
                else:
                    #–§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Å—Ü–µ–Ω—É
                    merged_scenes.append([current_scene[0], current_scene[1]])
                    num_audio_scene += 1
                    current_scene = [current_scene[1] + 0.01,current_scene[1] + 0.01]
        else:
            merged_scenes = video_scenes
        if len(subtitle_scenes) > 0:
            while num_video_scene < len(video_scenes) and num_subtitle_scene < len(subtitle_scenes):
                if subtitle_scenes[num_subtitle_scene][1] > video_scenes[num_video_scene][1]:
                    #–∫–æ–Ω–µ—Ü –≤–∏–¥–µ–æ—Å—Ü–µ–Ω—ã –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∞—É–¥–∏–æ—Å—Ü–µ–Ω–µ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                elif subtitle_scenes[num_subtitle_scene][1] > video_scenes[num_video_scene][1] - overlap_threshold:
                    #–∫–æ–Ω–µ—Ü –≤–∏–¥–µ–æ—Å—Ü–µ–Ω—ã –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –≥—Ä–∞–Ω–∏—Ü—É –∞—É–¥–∏–æ—Å—Ü–µ–Ω—ã –Ω–∞ –¥–æ–ø—É—Å—Ç–∏–º–æ–µ –≤—Ä–µ–º—è - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                    num_subtitle_scene += 1
                    if current_scene[1] - current_scene[0] > min_scene_duration:
                        merged_scenes.append([current_scene[0], current_scene[1]])
                        current_scene = [current_scene[1] + 0.01,current_scene[1] + 0.01]
                elif subtitle_scenes[num_subtitle_scene][1] >= video_scenes[num_video_scene][0] + overlap_threshold:
                    #–ë–æ–ª—å—à–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≥—Ä–∞–Ω–∏—Ü - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                    num_subtitle_scene += 1
                elif current_scene[1] - current_scene[0] < min_scene_duration:
                    #–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Å—Ü–µ–Ω–∞ - –æ–±—ä–µ–¥–∏–Ω—è–µ–º
                    current_scene[1] = video_scenes[num_video_scene][1]
                    num_video_scene += 1
                    if subtitle_scenes[num_subtitle_scene][1] < video_scenes[num_video_scene][0]:
                        num_subtitle_scene += 1
                else:
                    #–§–∏–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Å—Ü–µ–Ω—É
                    merged_scenes.append([current_scene[0], current_scene[1]])
                    num_subtitle_scene += 1
                    current_scene = [current_scene[1] + 0.01,current_scene[1] + 0.01]

        merged_scenes.append([current_scene[0], current_scene[1]])
        #–ü–æ—Å–ª–µ–¥–Ω—è—è —Å—Ü–µ–Ω–∞
        if current_scene[1] < video_scenes[-1][1]:
            if video_scenes[-1][1] - current_scene[1] > min_scene_duration:
                merged_scenes.append([current_scene[0], video_scenes[-1][1]])
            else:
                merged_scenes[-1][1] = video_scenes[-1][1]





        ''' 
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ü–µ–Ω—É
        if current_scene['end'] - current_scene['start'] >= min_scene_duration:
            merged_scenes.append((current_scene['start'], video_scenes[-1][1])) #–î–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –æ—à–∏–±–∫–∏ —Ä–∞–±–æ—Ç—ã —Å –≤–∏–¥–µ–æ

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ü–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        merged_scenes = [(start, end) for start, end in merged_scenes
                         if end - start >= min_scene_duration]
        '''
        return merged_scenes

    def _draw_results(self, frame: np.ndarray, tracked_boxes: pd.DataFrame,
                      frame_num: int) -> np.ndarray:
        """–û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∫–∞–¥—Ä–µ"""
        result_frame = frame.copy()

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if frame_num % 30 == 0:  # –ö–∞–∂–¥—ã–µ 30 –∫–∞–¥—Ä–æ–≤
            logger.debug(f"üé® –û—Ç—Ä–∏—Å–æ–≤–∫–∞: {len(tracked_boxes)} –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞–¥—Ä–µ {frame_num}")

        # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–æ–∫ —Å —Ä–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω—ã–º–∏ ID
        for row in tracked_boxes.itertuples():
            _,obj_id,_,shape,_,_ = row
            x1, y1, x2, y2 = shape
            if obj_id == None:
                continue
            color = self._get_color_for_id(obj_id)

            # –†–∞–º–∫–∞
            cv2.rectangle(result_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)

            # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            #text = f"ID: {obj_id}"
            text = self.names[obj_id] if obj_id in self.names else f"ID: {obj_id}"
            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
            cv2.rectangle(result_frame, (int(x1), int(y1)),
                          (int(x1)+text_width+10, int(y1)+text_height+10), color, -1)

            # –¢–µ–∫—Å—Ç ID
            cv2.putText(result_frame, text, (int(x1)+5, int(y1)+20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            #conf_text = f"{conf:.2f}"
            #cv2.putText(result_frame, conf_text, (int(x1), int(y2)-5),
            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return result_frame

    def _get_color_for_id(self, obj_id: int) -> Tuple[int, int, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è ID –æ–±—ä–µ–∫—Ç–∞"""
        if obj_id not in self.color_cache:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ ID
            hue = (obj_id * 137.5) % 360  # –ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HSV –≤ BGR
            h = hue / 60
            i = int(h)
            f = h - i
            p = 0
            q = 255 * (1 - f)
            t = 255 * f

            if i == 0:
                r, g, b = 255, t, p
            elif i == 1:
                r, g, b = q, 255, p
            elif i == 2:
                r, g, b = p, 255, t
            elif i == 3:
                r, g, b = p, q, 255
            elif i == 4:
                r, g, b = t, p, 255
            else:
                r, g, b = 255, p, q

            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —è—Ä–∫–æ—Å—Ç—å –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
            brightness = 0.8
            r = min(255, int(r * brightness))
            g = min(255, int(g * brightness))
            b = min(255, int(b * brightness))

            self.color_cache[obj_id] = (b, g, r)  # BGR —Ñ–æ—Ä–º–∞—Ç –¥–ª—è OpenCV

        return self.color_cache[obj_id]

    def read_srt_file(self,file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            with open(file_path, 'r', encoding='cp1251') as file:
                return file.read()

    def parse_srt(self,srt_text: str) -> List[SubtitleLine]:
        subtitles = []
        blocks = re.split(r'\n\s*\n', srt_text.strip())

        for block in tqdm.tqdm(blocks):
            lines = block.strip().split('\n')
            if len(lines) < 3:
                continue

            try:
                index = int(lines[0])
                time_match = re.match(r'(\d{2}):(\d{2}):(\d{2}),(\d{3}) --> (\d{2}):(\d{2}):(\d{2}),(\d{3})', lines[1])
                if not time_match:
                    continue

                h1, m1, s1, ms1 = map(int, time_match.groups()[:4])
                h2, m2, s2, ms2 = map(int, time_match.groups()[4:8])
                start = h1 * 3600 + m1 * 60 + s1 + ms1 / 1000
                end = h2 * 3600 + m2 * 60 + s2 + ms2 / 1000

                text = '\n'.join(lines[2:])
                doc = self.nlp(text)  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é spaCy
                subtitles.append(SubtitleLine(index, start, end, text, doc))
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–ª–æ–∫–∞: {e}")
                continue

        return subtitles

    def calculate_scene_metrics(self, scene: List[SubtitleLine]) -> Tuple[float, float]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –∏ —Å—Ä–µ–¥–Ω—é—é —Å—Ö–æ–∂–µ—Å—Ç—å —Ä–µ–ø–ª–∏–∫"""
        if not scene:
            return 0.0, 0.0

        durations = [sub.end - sub.start for sub in scene]
        similarities = []

        for i in range(1, len(scene)):
            sim = scene[i-1].doc.similarity(scene[i].doc)
            similarities.append(sim)

        total_duration = scene[-1].end - scene[0].start
        avg_similarity = mean(similarities) if similarities else 1.0

        return total_duration, avg_similarity



    def group_into_scenes(self, subtitles: List[SubtitleLine]) -> List[List[SubtitleLine]]:
        if not subtitles:
            return []

        # –°–Ω–∞—á–∞–ª–∞ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–æ—Å—Ç—ã–º –ø—Ä–∞–≤–∏–ª–∞–º
        initial_scenes = []
        current_scene = [subtitles[0]]

        for prev_sub, curr_sub in zip(subtitles, subtitles[1:]):
            time_gap = curr_sub.start - prev_sub.end
            similarity = prev_sub.doc.similarity(curr_sub.doc) if prev_sub.doc and curr_sub.doc else 0

            if time_gap > WEIGHTS['sub_time_gap'] or similarity < WEIGHTS['sub_similarity']:
                initial_scenes.append(current_scene)
                current_scene = [curr_sub]
            else:
                current_scene.append(curr_sub)

        initial_scenes.append(current_scene)

        # –ó–∞—Ç–µ–º –æ–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å—Ü–µ–Ω—ã
        merged_scenes = []

        for scene in initial_scenes:
            if not merged_scenes:
                merged_scenes.append(scene)
                continue

            if self.should_merge_scenes(merged_scenes[-1], scene):
                merged_scenes[-1].extend(scene)
            else:
                merged_scenes.append(scene)

        return merged_scenes

    def should_merge_scenes(self, prev_scene: List[SubtitleLine], current_scene: List[SubtitleLine]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Å—Ü–µ–Ω—ã"""
        if not prev_scene or not current_scene:
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞–º–∏
        time_gap = current_scene[0].start - prev_scene[-1].end

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ä–µ–ø–ª–∏–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π —Å—Ü–µ–Ω—ã –∏ –ø–µ—Ä–≤–æ–π —Ç–µ–∫—É—â–µ–π
        similarity = prev_scene[-1].doc.similarity(current_scene[0].doc)

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–π —Å—Ü–µ–Ω—ã
        merged_duration = current_scene[-1].end - prev_scene[0].start

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º, –µ—Å–ª–∏:
        # 1. –ù–µ–±–æ–ª—å—à–æ–π –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä–∞–∑—Ä—ã–≤ –ò —Ö–æ—Ä–æ—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
        # 2. –ò–õ–ò –µ—Å–ª–∏ –æ–¥–Ω–∞ –∏–∑ —Å—Ü–µ–Ω —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è
        # 3. –ò –ø—Ä–∏ —ç—Ç–æ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–∞—è —Å—Ü–µ–Ω–∞ –Ω–µ —Å—Ç–∞–Ω–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–π
        return ((time_gap <= WEIGHTS['sub_time_gap'] and similarity >= WEIGHTS['sub_similarity']) or
                any(self.calculate_scene_metrics(s)[0] < WEIGHTS['min_duration'] for s in [prev_scene, current_scene])) and \
            merged_duration <= WEIGHTS['max_duration']

    def prepare_subtitles_for_final(subtitles: List[SubtitleLine]) -> List[dict]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å—É–±—Ç–∏—Ç—Ä—ã –¥–ª—è –ø–µ—Ä–µ–¥–∞—á–∏ –≤ final_scene_segmentation"""
        return [{'start': sub.start, 'end': sub.end, 'text': sub.text} for sub in subtitles]

    def analyze_subtitles(self, subtitles_path = None, video_path = None):
        subtitle_scenes, subtitles = None, None

        if subtitles_path and os.path.exists(subtitles_path):
            logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–Ω–µ—à–Ω–∏–π —Ñ–∞–π–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤: {subtitles_path}")
            file = self.read_srt_file(subtitles_path)
            subtitles = self.parse_srt(file)
            subtitle_scenes = self.group_into_scenes(subtitles)
        else:
            embedded_srt = self.extract_embedded_subtitles(video_path)
            if embedded_srt:
                logger.info("–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã")
                file = self.read_srt_file(subtitles_path)
                subtitles = self.parse_srt(file)
                subtitle_scenes = self.group_into_scenes(subtitles)
            else:
                logger.info("–°—É–±—Ç–∏—Ç—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∞–Ω–∞–ª–∏–∑ –±—É–¥–µ—Ç –±–µ–∑ —É—á–µ—Ç–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤")
        return subtitle_scenes

    def extract_embedded_subtitles(self, video_path, output_srt_path='embedded_subtitles.srt'):
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã –∏–∑ –≤–∏–¥–µ–æ—Ñ–∞–π–ª–∞ —Å –ø–æ–º–æ—â—å—é ffmpeg"""
        try:
            cmd = [
                'ffmpeg',
                '-i', video_path,
                '-map', '0:s:0',
                '-c:s', 'srt',
                output_srt_path
            ]
            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

            if os.path.exists(output_srt_path) and os.path.getsize(output_srt_path) > 0:
                logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã –≤ {output_srt_path}")
                return output_srt_path
            else:
                logger.info("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã")
        except subprocess.CalledProcessError as e:
            logger.info(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ —Å—É–±—Ç–∏—Ç—Ä—ã: {e}")
        except Exception as e:
            logger.info(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Å—É–±—Ç–∏—Ç—Ä–æ–≤: {e}")
        return None