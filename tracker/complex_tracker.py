import os
import cv2
import pandas as pd
import dlib
import tqdm
import numpy as np
from typing import List, Tuple, Optional, Dict, Any

# –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
from moviepy import VideoFileClip
import scenedetect
from scenedetect.detectors import ContentDetector
from scenedetect.scene_manager import SceneManager

# –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
import librosa
import librosa.display
#from pydub import AudioSegment
#import speech_recognition as sr

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤


# –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤
from ultralytics import YOLO

from face_recognition import FaceRecognition

from .integrated_tracker_optimized import OptimizedBoundingBoxTracker

script_dir = os.path.dirname(os.path.abspath(__file__))

import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComplexVideoProcessor:
    def __init__(self, model_path: str = script_dir + "/yolov8n.pt", detector = 'mmod'):
        self.model = YOLO(model_path)
        self.color_cache = {}  # –ö—ç—à —Ü–≤–µ—Ç–æ–≤ –¥–ª—è ID
        self.face_recognition = FaceRecognition(detector,recognition_value = 0.4)
        self.face_recognition.load_dataset(tomemory = True)
        self.tracker = OptimizedBoundingBoxTracker()

    def video_short_tracker(self, clip, output_path: str, clipnum):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤–∏–¥–µ–æ —Å –¥–µ—Ç–µ–∫—Ü–∏–µ–π –∏ —Ç—Ä–µ–∫–æ–º–∏–Ω–≥–æ–º –æ–±—ä–µ–∫—Ç–æ–≤"""

        logger.info(f"üé¨ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ: {clip.n_frames} –∫–∞–¥—Ä–æ–≤, {clip.fps:.1f} FPS")
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, clip.fps, clip.size)

        self.names = {}
        #window = dlib.image_window()
        shapes_df = pd.DataFrame(columns=['id', 'frame', 'shape'])
        for frame_count in tqdm.tqdm(range(clip.n_frames)):
            frame = clip.get_frame(frame_count/clip.fps)

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–µ–∫–∏–Ω–≥ —Å YOLOv8
            results = self.model.track(frame, persist=True, tracker="bytetrack.yaml", verbose=False)

            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            #annotated_frame = results[0].plot()

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–π –ª—é–¥–µ–π
            detections = []

            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        if box.cls == 0 and box.id != None:  # –∫–ª–∞—Å—Å "person"
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            conf = box.conf[0].cpu().numpy()
                            #print(result.boxes.id)
                            detections.append((x1, y1, x2, y2, conf,int(box.id)))

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–µ—Ä–∞
            #tracked_boxes = self.tracker.update(detections, frame_count)
            tracked_boxes = detections if len(detections) > 0 else []
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ü
            if tracked_boxes:
                for box in tracked_boxes:
                    x1, y1, x2, y2 = box[:4]
                    x1 = int(x1)
                    y1 = int(y1)
                    x2 = int(x2)
                    y2 = int(y2)
                    obj_id = box[5]

                    if obj_id not in self.names:
                        #–≤—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–µ–Ω—è–µ–º BGR –Ω–∞ RGB
                        human_img = np.array(frame[y1:y2, x1:x2], dtype=np.uint8)

                        #window.set_image(human_img)
                        #–°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        #if human_img is not None:
                        #    human_img = cv2.cvtColor(human_img, cv2.COLOR_BGR2RGB)
                        #    cv2.imwrite(f"{time.time()}-{obj_id}.jpg", human_img)
                        name = self.face_recognition.find_name(human_img)
                        #window.wait_for_keypress(' ')
                        if name != "Unknown":
                            self.names[obj_id] = name
                            #window.set_title(f"{name} ({obj_id})")
                            logging.info(f"üë§ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –ª–∏—Ü–æ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {obj_id}: {self.names[obj_id]}")
                        #else:
                            #window.set_title('Unknown')

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–∫–∞—Ö
            if len(detections) > 0:
                for det in detections:
                    x1 = int(det[0])
                    y1 = int(det[1])
                    x2 = int(det[2])
                    y2 = int(det[3])
                    track_id = det[5]
                    shapes_df.loc[len(shapes_df)] = [int(track_id),frame_count,(x1, y1, x2, y2)]

        for frame_count in tqdm.tqdm(range(clip.n_frames)):
            # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            frame = clip.get_frame(frame_count/clip.fps)[:,:,::-1]
            boxes = shapes_df[shapes_df['frame'] == frame_count]
            processed_frame = self._draw_results(frame, boxes, frame_count, clip.fps)
            # –ó–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞
            out.write(processed_frame)

        out.release()

        return shapes_df

    def process_video(self, video_path: str, output_path: str):
        self.video_path = video_path
        self.output_path = output_path if output_path else ""
        if not os.path.exists(self.video_path):
            logger.error(f"üö® –í–∏–¥–µ–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {self.video_path}")
            return
        self.clip = VideoFileClip(self.video_path)
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è
        video_weight = 0.7  # –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è –≤–∏–¥–µ–æ —Å—Ü–µ–Ω
        audio_weight = 0.3  # –º–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è –∞—É–¥–∏–æ —Å—Ü–µ–Ω
        min_duration = 3.0  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        max_duration = 60.0 # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        merged_scenes = None
        video_scenes = self.detect_video_scenes(video_path)
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(video_scenes)} —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        for i, (start, end) in enumerate(video_scenes):
            logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫")
        if self.clip.audio is not None:
            self.clip.audio.write_audiofile(output_path + '/sound.wav', codec='pcm_s16le')
            audio_features = self.analyze_audio()
            audio_scenes = self.detect_audio_scenes(audio_features)
            logger.info(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(audio_scenes)} —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ –∞–Ω–∞–ª–∏–∑–∞:")
            for i, (start, end) in enumerate(audio_scenes):
                logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫")
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ü–µ–Ω—ã —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            merged_scenes = self.merge_scenes(video_scenes, audio_scenes,
                                         video_weight=video_weight,
                                         audio_weight=audio_weight,
                                         min_scene_duration=min_duration,
                                         max_scene_duration=max_duration)
            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(merged_scenes)} —Å—Ü–µ–Ω –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:")
            for i, (start, end) in enumerate(merged_scenes):
                logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {end-start:.2f} —Å–µ–∫)")

        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —à–æ—Ä—Ç—ã
        shorts_path = output_path + '/shorts'
        prefix = 'input_short_'
        self.save_scenes_as_videos(video_path, self.clip.fps, video_scenes, shorts_path, prefix)
        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ü–µ–Ω—ã
        if merged_scenes:
            scene_path = output_path + '/scenes'
            prefix = 'input_scene_'
            self.save_scenes_as_videos(video_path, self.clip.fps, merged_scenes, scene_path,prefix)

        #–æ–±—Ä–∞–±–æ—Ç–∫–∞ —à–æ—Ä—Ç–æ–≤
        if not os.path.exists(output_path + f'/shaped_shorts'):
            os.makedirs(output_path + f'/shaped_shorts')
        for i, (start, end) in enumerate(video_scenes):
            short_clip = self.clip.subclipped(start + 2/self.clip.fps, end)
            self.video_short_tracker(short_clip, output_path + f'/shaped_shorts/input_debug_{i}.mp4', i)
        self.clip.close()

    def save_scenes_as_videos(self, video_path, fps, final_scenes, output_dir='./scenes',prefix:str = ''):
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)

        for i, scene in enumerate(final_scenes):
            output_path = os.path.join(output_dir, f"{prefix}{i+1}.mp4")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º moviepy –¥–ª—è –≤—ã—Ä–µ–∑–∞–Ω–∏—è —Å—Ü–µ–Ω—ã
            clip = VideoFileClip(video_path).subclipped(scene[0] + 2/fps, scene[1])
            clip.write_videofile(output_path, codec='libx264', audio_codec='aac')
            clip.close()
            logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ü–µ–Ω–∞ {i+1} –≤ {output_path}")

    def analyze_audio(self, frame_length=2048, hop_length=512):
        if self.clip.audio is None:
            logger.error("üö® –í–∏–¥–µ–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—É–¥–∏–æ")
            return
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ
        y, sr = librosa.load(self.output_path + '/sound.wav')
        #fps = self.clip.audio.fps
        #y, sr = np.array(self.clip.audio.to_soundarray(fps = fps)*(2**16), dtype = "int16"), fps

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ RMS —ç–Ω–µ—Ä–≥–∏–∏
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        rms_normalized = (rms - np.min(rms)) / (np.max(rms) - np.min(rms))

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞
        spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr, n_fft=frame_length, hop_length=hop_length)[0]
        spectral_centroid_normalized = (spectral_centroid - np.min(spectral_centroid)) / (np.max(spectral_centroid) - np.min(spectral_centroid))

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ zero crossing rate
        zcr = librosa.feature.zero_crossing_rate(y, frame_length=frame_length, hop_length=hop_length)[0]

        # –í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—Å—å –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        times = librosa.times_like(rms, sr=sr, hop_length=hop_length, n_fft=frame_length)

        return {
            'times': times,
            'rms': rms_normalized,
            'spectral_centroid': spectral_centroid_normalized,
            'zcr': zcr
        }

    def detect_audio_scenes(self, audio_features, rms_threshold=0.3, centroid_threshold=0.4, min_scene_duration=3.0):
        times = audio_features['times']
        rms = audio_features['rms']
        spectral_centroid = audio_features['spectral_centroid']

        scenes = []
        current_scene_start = 0.0

        for i in range(1, len(times)):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ RMS –∏ —Å–ø–µ–∫—Ç—Ä–∞–ª—å–Ω–æ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥–µ
            rms_change = abs(rms[i] - rms[i-1])
            centroid_change = abs(spectral_centroid[i] - spectral_centroid[i-1])

            # –ï—Å–ª–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ, —Å—á–∏—Ç–∞–µ–º —ç—Ç–æ –≥—Ä–∞–Ω–∏—Ü–µ–π —Å—Ü–µ–Ω—ã
            if rms_change > rms_threshold or centroid_change > centroid_threshold:
                scene_duration = times[i] - current_scene_start
                if scene_duration >= min_scene_duration:
                    scenes.append((current_scene_start, times[i]))
                    current_scene_start = times[i]

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ü–µ–Ω—É
        if current_scene_start < times[-1]:
            scenes.append((current_scene_start, times[-1]))

        return scenes

    def detect_video_scenes(self,video_path, threshold=30.0):
        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä —Å—Ü–µ–Ω –∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä
        video = scenedetect.VideoManager([video_path])
        scene_manager = SceneManager()
        scene_manager.add_detector(ContentDetector(threshold=threshold))

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
        video.set_downscale_factor()
        video.start()
        scene_manager.detect_scenes(frame_source=video)

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω
        scene_list = scene_manager.get_scene_list()

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ–∫—É–Ω–¥—ã
        scenes = []
        for scene in scene_list:
            start_time = scene[0].get_seconds()
            end_time = scene[1].get_seconds()
            scenes.append((start_time, end_time))

        return scenes

    def merge_scenes(self,video_scenes, audio_scenes,
                     video_weight=0.7, audio_weight=0.3,
                     min_scene_duration=3.0, max_scene_duration=120.0,
                     overlap_threshold=2.0):
        """
        –£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è —Å—Ü–µ–Ω —Å —É—á–µ—Ç–æ–º:
        - –≤–µ—Å–æ–≤—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–∏–¥–µ–æ –∏ –∞—É–¥–∏–æ
        - –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å—Ü–µ–Ω
        - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –≥—Ä–∞–Ω–∏—Ü
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏ –≤–∑–≤–µ—à–∏–≤–∞–µ–º —Å—Ü–µ–Ω—ã
        weighted_scenes = []

        # –î–æ–±–∞–≤–ª—è–µ–º –≤–∏–¥–µ–æ —Å—Ü–µ–Ω—ã —Å –≤–µ—Å–æ–º
        for start, end in video_scenes:
            duration = end - start
            if duration >= min_scene_duration:
                weighted_scenes.append({
                    'start': start,
                    'end': end,
                    'weight': video_weight,
                    'type': 'video'
                })

        # –î–æ–±–∞–≤–ª—è–µ–º –∞—É–¥–∏–æ —Å—Ü–µ–Ω—ã —Å –≤–µ—Å–æ–º
        for start, end in audio_scenes:
            duration = end - start
            if duration >= min_scene_duration:
                weighted_scenes.append({
                    'start': start,
                    'end': end,
                    'weight': audio_weight,
                    'type': 'audio'
                })

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ —Å—Ü–µ–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
        weighted_scenes.sort(key=lambda x: x['start'])

        if not weighted_scenes:
            return []

        # –ê–ª–≥–æ—Ä–∏—Ç–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        merged_scenes = []
        current_scene = weighted_scenes[0].copy()

        for scene in weighted_scenes[1:]:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –∏–ª–∏ –±–ª–∏–∑–æ—Å—Ç—å —Å—Ü–µ–Ω
            scene_overlap = (scene['start'] <= current_scene['end'] + overlap_threshold)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            duration_exceeded = (scene['end'] - current_scene['start']) > max_scene_duration

            # –ï—Å–ª–∏ —Å—Ü–µ–Ω—ã –ø–µ—Ä–µ—Å–µ–∫–∞—é—Ç—Å—è –∏ –Ω–µ –ø—Ä–µ–≤—ã—à–∞—é—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            if scene_overlap and not duration_exceeded:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ü–µ–Ω—ã —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤
                if scene['type'] == 'video' and current_scene['type'] != 'video':
                    current_scene['end'] = scene['end']
                    current_scene['weight'] += scene['weight']
                    current_scene['type'] = 'mixed'
                elif scene['weight'] > current_scene['weight']:
                    current_scene['end'] = scene['end']
                    current_scene['weight'] += scene['weight']
                    if scene['type'] != current_scene['type']:
                        current_scene['type'] = 'mixed'
                else:
                    current_scene['end'] = max(current_scene['end'], scene['end'])
                    current_scene['weight'] += scene['weight'] * 0.5  # –º–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ü–µ–Ω—É –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é
                if current_scene['end'] - current_scene['start'] >= min_scene_duration:
                    merged_scenes.append((current_scene['start'], current_scene['end']))
                current_scene = scene.copy()

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ü–µ–Ω—É
        if current_scene['end'] - current_scene['start'] >= min_scene_duration:
            merged_scenes.append((current_scene['start'], current_scene['end']))

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ü–µ–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
        merged_scenes = [(start, end) for start, end in merged_scenes
                         if end - start >= min_scene_duration]

        return merged_scenes

    def _draw_results(self, frame: np.ndarray, tracked_boxes: pd.DataFrame,
                      frame_num: int, fps: float) -> np.ndarray:
        """–û—Ç—Ä–∏—Å–æ–≤—ã–≤–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –∫–∞–¥—Ä–µ"""
        result_frame = frame.copy()

        # –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        if frame_num % 30 == 0:  # –ö–∞–∂–¥—ã–µ 30 –∫–∞–¥—Ä–æ–≤
            logger.info(f"üé® –û—Ç—Ä–∏—Å–æ–≤–∫–∞: {len(tracked_boxes)} –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞–¥—Ä–µ {frame_num}")

        # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ —Ä–∞–º–æ–∫ —Å —Ä–∞–∑–Ω–æ—Ü–≤–µ—Ç–Ω—ã–º–∏ ID
        for row in tracked_boxes.itertuples():
            _,obj_id,_,shape = row
            x1, y1, x2, y2 = shape
            if obj_id == None:
                continue
            color = self._get_color_for_id(obj_id)

            # –†–∞–º–∫–∞
            cv2.rectangle(result_frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 3)

            # –§–æ–Ω –¥–ª—è —Ç–µ–∫—Å—Ç–∞
            #text = f"ID: {obj_id}"
            text = self.names[obj_id] if obj_id in self.names else f"ID: {obj_id}"
            (text_width, text_height), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
            cv2.rectangle(result_frame, (int(x1), int(y1)),
                          (int(x1)+text_width+10, int(y1)+text_height+10), color, -1)

            # –¢–µ–∫—Å—Ç ID
            cv2.putText(result_frame, text, (int(x1)+5, int(y1)+20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            #conf_text = f"{conf:.2f}"
            #cv2.putText(result_frame, conf_text, (int(x1), int(y2)-5),
            #            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        return result_frame

    def _get_color_for_id(self, obj_id: int) -> Tuple[int, int, int]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ü–≤–µ—Ç –¥–ª—è ID –æ–±—ä–µ–∫—Ç–∞"""
        if obj_id not in self.color_cache:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ ID
            hue = (obj_id * 137.5) % 360  # –ó–æ–ª–æ—Ç–æ–µ —Å–µ—á–µ–Ω–∏–µ –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HSV –≤ BGR
            h = hue / 60
            i = int(h)
            f = h - i
            p = 0
            q = 255 * (1 - f)
            t = 255 * f

            if i == 0:
                r, g, b = 255, t, p
            elif i == 1:
                r, g, b = q, 255, p
            elif i == 2:
                r, g, b = p, 255, t
            elif i == 3:
                r, g, b = p, q, 255
            elif i == 4:
                r, g, b = t, p, 255
            else:
                r, g, b = 255, p, q

            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —è—Ä–∫–æ—Å—Ç—å –¥–ª—è –ª—É—á—à–µ–π –≤–∏–¥–∏–º–æ—Å—Ç–∏
            brightness = 0.8
            r = min(255, int(r * brightness))
            g = min(255, int(g * brightness))
            b = min(255, int(b * brightness))

            self.color_cache[obj_id] = (b, g, r)  # BGR —Ñ–æ—Ä–º–∞—Ç –¥–ª—è OpenCV

        return self.color_cache[obj_id]

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    processor = ComplexVideoProcessor()
    df = processor.video_short_tracker("C:/Users/above/IdeaProjects/video/Video_Samples/in2.mp4", "", "", "output_video_optimized.mp4")
    print(df)

if __name__ == "__main__":
    main()