import os
import cv2
import pandas as pd
import tqdm
import numpy as np
from typing import List, Tuple, Optional, Dict, Any

# –í–∏–¥–µ–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
from moviepy import VideoFileClip
import scenedetect
from scenedetect.detectors import ContentDetector
from scenedetect.scene_manager import SceneManager

# –ê—É–¥–∏–æ –æ–±—Ä–∞–±–æ—Ç–∫–∞
import librosa
import librosa.display
#from pydub import AudioSegment
#import speech_recognition as sr

# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—É–±—Ç–∏—Ç—Ä–æ–≤


# –î–µ—Ç–µ–∫—Ü–∏—è –∏ —Ç—Ä–µ–∫–∏–Ω–≥ –æ–±—ä–µ–∫—Ç–æ–≤
from ultralytics import YOLO

from face_recognition import FaceRecognition

from .base_tracker import BaseVideoProcessor

script_dir = os.path.dirname(os.path.abspath(__file__))

import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComplexVideoProcessor(BaseVideoProcessor):
    def __init__(self, model_path: str = script_dir + "/yolov8n.pt", detector = 'mmod', force_update = False):
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
        super().__init__(model_path, detector, force_update)


    def video_short_pretracker(self, clip, clipnum):
        #–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥ –ø–æ –≤–∏–¥–µ–æ –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –±–∞–∑—É –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä–æ–≤ –æ–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –∞–∫—Ç–µ—Ä–æ–≤ –∏–º–µ–Ω–Ω–æ –∏–∑ —ç—Ç–æ–≥–æ –≤–∏–¥–µ–æ
        logger.info(f"üé¨ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —à–æ—Ç–∞ {clipnum}: {clip.n_frames} –∫–∞–¥—Ä–æ–≤, {clip.fps:.1f} FPS")

        self.names = {}
        #window = dlib.image_window()
        shapes_df = pd.DataFrame(columns=['id', 'frame', 'shape', 'face_desc'])
        for frame_count in range(clip.n_frames):
            frame = clip.get_frame(frame_count/clip.fps)

            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ç—Ä–µ–∫–∏–Ω–≥ —Å YOLOv8
            results = self.model.track(frame, stream=True, persist=True, tracker="botsort.yaml", verbose=False)

            # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ü–∏–π –ª—é–¥–µ–π
            detections = []

            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        if box.cls == 0 and box.id != None:  # –∫–ª–∞—Å—Å "person"
                            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                            conf = box.conf[0].cpu().numpy()
                            #print(result.boxes.id)
                            detections.append([x1, y1, x2, y2, conf,int(box.id)])

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–µ—Ä–∞
            #tracked_boxes = self.tracker.update(detections, frame_count)
            tracked_boxes = detections if len(detections) > 0 else []
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ü

            if tracked_boxes:
                for box in tracked_boxes:
                    x1, y1, x2, y2 = box[:4]
                    x1 = int(x1)
                    y1 = int(y1)
                    x2 = int(x2)
                    y2 = int(y2)
                    obj_id = box[5]

                    if obj_id not in self.names:
                        #–≤—ã–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –º–µ–Ω—è–µ–º BGR –Ω–∞ RGB
                        human_img = np.array(frame[y1:y2, x1:x2], dtype=np.uint8)

                        #window.set_image(human_img)
                        face = self.face_recognition.detector.shape_of_image(human_img)
                        if face is None:
                            description = None
                        else:
                            description = self.face_recognition.facerec.compute_face_descriptor(human_img, face)
                        box.append(face)
                        if description is not None:
                            name = self.face_recognition.find_name_desc(description)
                            if name != "Unknown":
                                if len(self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] == name]) < 10:
                                    #print(len(self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] == name]))
                                    self.face_recognition.add_face_desc(description, name)
                                    logging.info(f"üë§ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ª–∏—Ü–æ –¥–ª—è –æ–±—ä–µ–∫—Ç–∞ {name}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–∫–∞—Ö
            if len(tracked_boxes) > 0:
                for det in tracked_boxes:
                    x1 = int(det[0])
                    y1 = int(det[1])
                    x2 = int(det[2])
                    y2 = int(det[3])
                    track_id = det[5]
                    shapes_df.loc[len(shapes_df)] = [int(track_id),frame_count,(x1, y1, x2, y2),det[6]]

        return shapes_df

    def process_video(self, video_path: str, output_path: str):
        self.video_path = video_path
        self.output_path = output_path if output_path else ""
        if not os.path.exists(self.video_path):
            logger.error(f"üö® –í–∏–¥–µ–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {self.video_path}")
            return
        self.clip = VideoFileClip(self.video_path)
        fps = self.clip.n_frames / self.clip.duration
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–∞—à–µ–≥–æ —Å–ª—É—á–∞—è
        video_weight = 0.7  # –±–æ–ª—å—à–∏–π –≤–µ—Å –¥–ª—è –≤–∏–¥–µ–æ —Å—Ü–µ–Ω
        audio_weight = 0.3  # –º–µ–Ω—å—à–∏–π –≤–µ—Å –¥–ª—è –∞—É–¥–∏–æ —Å—Ü–µ–Ω
        min_duration = 10.0  # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        max_duration = 600.0 # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        merged_scenes = None
        video_scenes = self.detect_video_scenes(video_path)
        logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(video_scenes)} —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∏–¥–µ–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:")
        for i, (start, end) in enumerate(video_scenes):
            logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫")
        if self.clip.audio is not None:
            self.clip.audio.write_audiofile(output_path + '/sound.wav', codec='pcm_s16le')
            audio_features = self.analyze_audio()
            audio_scenes = self.detect_audio_scenes(audio_features)
            logger.info(f"\n–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(audio_scenes)} —Å—Ü–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—É–¥–∏–æ –∞–Ω–∞–ª–∏–∑–∞:")
            for i, (start, end) in enumerate(audio_scenes):
                logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫")
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å—Ü–µ–Ω—ã —Å –Ω–æ–≤—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            merged_scenes = self.merge_scenes(video_scenes, audio_scenes,
                                         video_weight=video_weight,
                                         audio_weight=audio_weight,
                                         min_scene_duration=min_duration,
                                         max_scene_duration=max_duration)
            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(merged_scenes)} —Å—Ü–µ–Ω –ø–æ—Å–ª–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è:")
            for i, (start, end) in enumerate(merged_scenes):
                logger.info(f"–°—Ü–µ–Ω–∞ {i+1}: {start:.2f} - {end:.2f} —Å–µ–∫ (–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {end-start:.2f} —Å–µ–∫)")

        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —à–æ—Ç—ã
        shorts_path = output_path + '/shorts'
        prefix = 'input_short_'
        self.save_scenes_as_videos(video_path, fps, video_scenes, shorts_path, prefix)
        '''results_list = self.model.track(video_path, stream=True, persist=True, tracker="bytetrack.yaml", verbose=False)
        for ind,results in enumerate(results_list):
            if len(results) > 0:
                id = results[0].boxes.id
            else:
                id = None
            print(ind,id)'''
        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ü–µ–Ω—ã
        if merged_scenes:
            scene_path = output_path + '/scenes'
            prefix = 'input_scene_'
            self.save_scenes_as_videos(video_path, fps, merged_scenes, scene_path,prefix)

        #–æ–±—Ä–∞–±–æ—Ç–∫–∞ —à–æ—Ç–æ–≤
        if not os.path.exists(output_path + f'/shaped_shorts'):
            os.makedirs(output_path + f'/shaped_shorts')
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —à–æ—Ç–æ–≤, 1 –ø—Ä–æ—Ö–æ–¥...")
        for i, (start, end) in enumerate(video_scenes):
            short_clip = self.clip.subclipped(start + 1/fps, end)
            df = self.video_short_pretracker(short_clip, i)
            self.shapes_list.append(df)
        #–£–¥–∞–ª—è–µ–º –∏–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–ø–∏—Å–∏, —Å –∏–º–µ–Ω–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–æ–∑–Ω–∞–Ω—ã –º–µ–Ω–µ–µ, —á–µ–º –≤ 10 –∫–∞–¥—Ä–∞—Ö
        for name in self.face_recognition.local_dataset['name'].unique():
            if len(self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] == name]) < 10:
                self.face_recognition.local_dataset = self.face_recognition.local_dataset[self.face_recognition.local_dataset['name'] != name]
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —à–æ—Ç–æ–≤, 2 –ø—Ä–æ—Ö–æ–¥...")
        for i, (start, end) in enumerate(video_scenes):
            short_clip = self.clip.subclipped(start + 1/fps, end)
            self.video_short_tracker(short_clip, output_path + f'/shaped_shorts/input_debug_{i}.mp4', i)
        self.clip.close()

        #–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        data = []
        prev_frame = 0

        for i, scene in enumerate(video_scenes):
            duration = (scene[1] - scene[0])
            change_frame = int(scene[1] * fps)
            data.append({
                'scene': i,
                'start_frame': prev_frame,
                'start_time': prev_frame / fps,
                'end_frame': change_frame,
                'end_time': change_frame / fps,
                'duration_seconds': duration
            })
            prev_frame = change_frame


        df = pd.DataFrame(data)
        scene_analysis_path = output_path + '/shot_analysis.csv'
        df.to_csv(scene_analysis_path, index=False)
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —à–æ—Ç–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scene_analysis_path}")

        data = []
        prev_frame = 0

        for i, scene in enumerate(merged_scenes):
            duration = (scene[1] - scene[0])
            change_frame = int(scene[1] * fps)
            data.append({
                'scene': i,
                'start_frame': prev_frame,
                'start_time': prev_frame / fps,
                'end_frame': change_frame,
                'end_time': change_frame / fps,
                'duration_seconds': duration
            })
            prev_frame = change_frame


        df = pd.DataFrame(data)
        scene_analysis_path = output_path + '/scene_analysis.csv'
        df.to_csv(scene_analysis_path, index=False)
        logger.info(f"üìä –ê–Ω–∞–ª–∏–∑ —Å—Ü–µ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {scene_analysis_path}")

    def detect_video_scenes(self,video_path, threshold=30.0):
        # –°–æ–∑–¥–∞–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä —Å—Ü–µ–Ω –∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä
        video = scenedetect.VideoManager([video_path])
        scene_manager = SceneManager()
        scene_manager.add_detector(ContentDetector(threshold=threshold))

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ
        video.set_downscale_factor()
        video.start()
        scene_manager.detect_scenes(frame_source=video)

        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å—Ü–µ–Ω
        scene_list = scene_manager.get_scene_list()

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ–∫—É–Ω–¥—ã
        scenes = []
        for scene in scene_list:
            start_time = scene[0].get_seconds()
            end_time = scene[1].get_seconds()
            scenes.append((start_time, end_time))

        return scenes

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    processor = ComplexVideoProcessor()
    processor.process_video("C:/Users/above/IdeaProjects/video/Video_Samples/in2.mp4",
                                 "C:/Users/above/IdeaProjects/video/Video_SamplesC:/Users/above/IdeaProjects/video/Video_Samples/video/Video_Samples/out")

if __name__ == "__main__":
    main()